---
title: Web-Search Ranking with Initialized Gradient Boosted Regression Trees
abstract: In May 2010 Yahoo! Inc. hosted the Learning to Rank Challenge. This paper
  summarizes the approach by the highly placed team Washington University in St. Louis.
  We investigate Random Forests (RF) as a low-cost alternative algorithm to Gradient
  Boosted Regression Trees (GBRT) (the de facto standard of web-search ranking). We
  demonstrate that it yields surprisingly accurate ranking results â€“ comparable to
  or better than GBRT. We combine the two algorithms by first learning a ranking function
  with RF and using it as initialization for GBRT. We refer to this setting as iGBRT.
  Following a recent discussion by Li et al. (2007), we show that the results of iGBRT can be improved
  upon even further when the web-search ranking task is cast as classification instead
  of regression. We provide an upper bound of the Expected Reciprocal Rank (Chapelle et al., 2009) in
  terms of classification error and demonstrate that iGBRT outperforms GBRT and RF
  on the Microsoft Learning to Rank and Yahoo Ranking Competition data sets with surprising
  consistency.
pdf: http://proceedings.mlr.press/v14/mohan11a/mohan11a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: mohan11a
month: 0
tex_title: Web-Search Ranking with Initialized Gradient Boosted Regression Trees
firstpage: 77
lastpage: 89
page: 77-89
order: 77
cycles: false
author:
- given: Ananth
  family: Mohan
- given: Zheng
  family: Chen
- given: Kilian
  family: Weinberger
date: 2011-01-26
address: Haifa, Israel
publisher: PMLR
container-title: Proceedings of the Learning to Rank Challenge
volume: '14'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 1
  - 26
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
